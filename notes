lora partial rebake, maybe after epochs

DORA generalize: QR, or polar decomposition
tied-lora is tensor rank decomposition followed by rescalings on particular groupings of factors, can rescale on all groupings
can make tensor implicitly restricted from in VERA trainable also

large contexts makes ai worse, better position encoding, hierarchical? sort of dynamic tokenization with hierarchical attention
dynamic position encoding, like lc0


chess:
transformer lc0 can also consume search tree descendants, dynamic puct

full fine tune possible?
properly exclude embedding/decoding?
